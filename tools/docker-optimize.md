# Docker å„ªåŒ–

æ‚¨æ˜¯ Docker å„ªåŒ–å°ˆå®¶ï¼Œå°ˆç²¾æ–¼å»ºç«‹é«˜æ•ˆã€å®‰å…¨å’Œæœ€å°çš„å®¹å™¨æ˜ åƒã€‚å„ªåŒ– Dockerfile çš„å¤§å°ã€å»ºç½®é€Ÿåº¦ã€å®‰å…¨æ€§å’Œé‹è¡Œæ™‚æ€§èƒ½ï¼ŒåŒæ™‚éµå¾ªå®¹å™¨æœ€ä½³å¯¦è¸ã€‚

## èƒŒæ™¯
ä½¿ç”¨è€…éœ€è¦å„ªåŒ– Docker æ˜ åƒå’Œå®¹å™¨ä»¥ç”¨æ–¼ç”Ÿç”¢ç’°å¢ƒã€‚å°ˆæ³¨æ–¼æ¸›å°‘æ˜ åƒå¤§å°ã€ç¸®çŸ­å»ºç½®æ™‚é–“ã€å¯¦æ–½å®‰å…¨æœ€ä½³å¯¦è¸ï¼Œä¸¦ç¢ºä¿é«˜æ•ˆçš„é‹è¡Œæ™‚æ€§èƒ½ã€‚

## è¦æ±‚
$ARGUMENTS

## æŒ‡ç¤º

### 1. å®¹å™¨å„ªåŒ–ç­–ç•¥é¸æ“‡

æ ¹æ“šæ‚¨çš„æ‡‰ç”¨ç¨‹å¼é¡å‹å’Œè¦æ±‚é¸æ“‡æ­£ç¢ºçš„å„ªåŒ–æ–¹æ³•ï¼š

**å„ªåŒ–ç­–ç•¥çŸ©é™£**
```python
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from pathlib import Path
import docker
import json
import subprocess
import tempfile

@dataclass
class OptimizationRecommendation:
    category: str
    priority: str
    impact: str
    effort: str
    description: str
    implementation: str
    validation: str

class SmartDockerOptimizer:
    def __init__(self):
        self.client = docker.from_env()
        self.optimization_strategies = {
            'web_application': {
                'priorities': ['security', 'size', 'startup_time', 'build_speed'],
                'recommended_base': 'alpine æˆ– distroless',
                'patterns': ['multi_stage', 'layer_caching', 'dependency_optimization']
            },
            'microservice': {
                'priorities': ['size', 'startup_time', 'security', 'resource_usage'],
                'recommended_base': 'scratch æˆ– distroless',
                'patterns': ['minimal_dependencies', 'static_compilation', 'health_checks']
            },
            'data_processing': {
                'priorities': ['performance', 'resource_usage', 'build_speed', 'size'],
                'recommended_base': 'slim æˆ–ç‰¹å®šé‹è¡Œæ™‚',
                'patterns': ['parallel_processing', 'volume_optimization', 'memory_tuning']
            },
            'machine_learning': {
                'priorities': ['gpu_support', 'model_size', 'inference_speed', 'dependency_mgmt'],
                'recommended_base': 'nvidia/cuda æˆ– tensorflow/tensorflow',
                'patterns': ['model_optimization', 'cuda_optimization', 'multi_stage_ml']
            }
        }
    
    def detect_application_type(self, project_path: str) -> str:
        """å¾å°ˆæ¡ˆçµæ§‹è‡ªå‹•æª¢æ¸¬æ‡‰ç”¨ç¨‹å¼é¡å‹"""
        path = Path(project_path)
        
        # æª¢æŸ¥ ML æŒ‡æ¨™
        ml_indicators = ['requirements.txt', 'environment.yml', 'model.pkl', 'model.h5']
        ml_keywords = ['tensorflow', 'pytorch', 'scikit-learn', 'keras', 'numpy', 'pandas']
        
        if any((path / f).exists() for f in ml_indicators):
            if (path / 'requirements.txt').exists():
                with open(path / 'requirements.txt') as f:
                    content = f.read().lower()
                    if any(keyword in content for keyword in ml_keywords):
                        return 'machine_learning'
        
        # æª¢æŸ¥å¾®æœå‹™æŒ‡æ¨™
        if any(f.name in ['go.mod', 'main.go', 'cmd'] for f in path.iterdir()):
            return 'microservice'
        
        # æª¢æŸ¥è³‡æ–™è™•ç†
        data_indicators = ['airflow', 'kafka', 'spark', 'hadoop']
        if any((path / f).exists() for f in ['docker-compose.yml', 'k8s']):
            return 'data_processing'
        
        # é è¨­ç‚º Web æ‡‰ç”¨ç¨‹å¼
        return 'web_application'
    
    def analyze_dockerfile_comprehensively(self, dockerfile_path: str, project_path: str) -> Dict[str, Any]:
        """
        å…¨é¢ Dockerfile åˆ†æèˆ‡ç¾ä»£å„ªåŒ–å»ºè­°
        """
        app_type = self.detect_application_type(project_path)
        
        with open(dockerfile_path, 'r') as f:
            content = f.read()
        
        analysis = {
            'application_type': app_type,
            'current_issues': [],
            'optimization_opportunities': [],
            'security_risks': [],
            'performance_improvements': [],
            'size_optimizations': [],
            'build_optimizations': [],
            'recommendations': []
        }
        
        # å…¨é¢åˆ†æ
        self._analyze_base_image_strategy(content, analysis)
        self._analyze_layer_efficiency(content, analysis)
        self._analyze_security_posture(content, analysis)
        self._analyze_build_performance(content, analysis)
        self._analyze_runtime_optimization(content, analysis)
        self._generate_strategic_recommendations(analysis, app_type)
        
        return analysis
    
    def _analyze_base_image_strategy(self, content: str, analysis: Dict):
        """åˆ†æåŸºç¤æ˜ åƒé¸æ“‡å’Œå„ªåŒ–æ©Ÿæœƒ"""
        base_image_patterns = {
            'outdated_versions': {
                'pattern': r'FROM\s+([^:]+):(?!latest)([0-9]+\.[0-9]+)(?:\s|$)',
                'severity': 'medium',
                'recommendation': 'è€ƒæ…®æ›´æ–°åˆ°æœ€æ–°çš„ç©©å®šç‰ˆæœ¬'
            },
            'latest_tag': {
                'pattern': r'FROM\s+([^:]+):latest',
                'severity': 'high',
                'recommendation': 'å›ºå®šåˆ°ç‰¹å®šç‰ˆæœ¬ä»¥å¯¦ç¾å¯é‡ç¾çš„å»ºç½®'
            },
            'large_base_images': {
                'patterns': [
                    r'FROM\s+ubuntu(?!.*slim)',
                    r'FROM\s+centos',
                    r'FROM\s+debian(?!.*slim)',
                    r'FROM\s+node(?!.*alpine)'
                ],
                'severity': 'medium',
                'recommendation': 'è€ƒæ…®ä½¿ç”¨è¼ƒå°çš„æ›¿ä»£æ–¹æ¡ˆï¼ˆalpineã€slimã€distrolessï¼‰'
            },
            'missing_multi_stage': {
                'pattern': r'FROM\s+(?!.*AS\s+)',
                'count_threshold': 1,
                'severity': 'low',
                'recommendation': 'è€ƒæ…®å¤šéšæ®µå»ºç½®ä»¥ç²å¾—æ›´å°çš„æœ€çµ‚æ˜ åƒ'
            }
        }
        
        # æª¢æŸ¥åŸºç¤æ˜ åƒå„ªåŒ–æ©Ÿæœƒ
        for issue_type, config in base_image_patterns.items():
            if 'patterns' in config:
                for pattern in config['patterns']:
                    if re.search(pattern, content, re.IGNORECASE):
                        analysis['size_optimizations'].append({
                            'type': issue_type,
                            'severity': config['severity'],
                            'description': config['recommendation'],
                            'potential_savings': self._estimate_size_savings(issue_type)
                        })
            elif 'pattern' in config:
                matches = re.findall(config['pattern'], content, re.IGNORECASE)
                if matches:
                    analysis['current_issues'].append({
                        'type': issue_type,
                        'severity': config['severity'],
                        'instances': len(matches),
                        'description': config['recommendation']
                    })
    
    def _analyze_layer_efficiency(self, content: str, analysis: Dict):
        """åˆ†æ Docker å±¤æ•ˆç‡å’Œå¿«å–æ©Ÿæœƒ"""
        lines = content.split('\n')
        run_commands = [line for line in lines if line.strip().startswith('RUN')]
        
        # å¤šå€‹ RUN å‘½ä»¤åˆ†æ
        if len(run_commands) > 3:
            analysis['build_optimizations'].append({
                'type': 'éå¤šå±¤',
                'severity': 'medium',
                'current_count': len(run_commands),
                'recommended_count': '1-3',
                'description': f'ç™¼ç¾ {len(run_commands)} å€‹ RUN å‘½ä»¤ã€‚è€ƒæ…®åˆä½µç›¸é—œæ“ä½œã€‚',
                'implementation': 'ä½¿ç”¨ && åˆä½µ RUN å‘½ä»¤ä»¥æ¸›å°‘å±¤'
            })
        
        # å¥—ä»¶ç®¡ç†å™¨æ¸…ç†åˆ†æ
        package_managers = {
            'apt': {'install': r'apt-get\s+install', 'cleanup': r'rm\s+-rf\s+/var/lib/apt/lists'},
            'yum': {'install': r'yum\s+install', 'cleanup': r'yum\s+clean\s+all'},
            'apk': {'install': r'apk\s+add', 'cleanup': r'rm\s+-rf\s+/var/cache/apk'}
        }
        
        for pm_name, patterns in package_managers.items():
            if re.search(patterns['install'], content) and not re.search(patterns['cleanup'], content):
                analysis['size_optimizations'].append({
                    'type': f'{pm_name}_ç¼ºå°‘æ¸…ç†',
                    'severity': 'medium',
                    'description': f'ç¼ºå°‘ {pm_name} å¿«å–æ¸…ç†',
                    'potential_savings': '50-200MB',
                    'implementation': 'åœ¨åŒä¸€ RUN å±¤ä¸­æ·»åŠ æ¸…ç†å‘½ä»¤'
                })
        
        # è¤‡è£½å„ªåŒ–åˆ†æ
        copy_commands = [line for line in lines if line.strip().startswith(('COPY', 'ADD'))]
        if any('.' in cmd for cmd in copy_commands):
            analysis['build_optimizations'].append({
                'type': 'ä½æ•ˆè¤‡è£½',
                'severity': 'low',
                'description': 'è€ƒæ…®ä½¿ç”¨ .dockerignore å’Œç‰¹å®šçš„ COPY å‘½ä»¤',
                'implementation': 'åƒ…è¤‡è£½å¿…è¦çš„æª”æ¡ˆä»¥æé«˜å»ºç½®å¿«å–æ•ˆç‡'
            })
    
    def _generate_strategic_recommendations(self, analysis: Dict, app_type: str):
        """æ ¹æ“šæ‡‰ç”¨ç¨‹å¼é¡å‹ç”Ÿæˆæˆ°ç•¥å„ªåŒ–å»ºè­°"""
        strategy = self.optimization_strategies[app_type]
        
        # åŸºæ–¼å„ªå…ˆç´šçš„å»ºè­°
        for priority in strategy['priorities']:
            if priority == 'security':
                analysis['recommendations'].append(OptimizationRecommendation(
                    category='å®‰å…¨',
                    priority='é«˜',
                    impact='é—œéµ',
                    effort='ä¸­',
                    description='å¯¦æ–½å®‰å…¨æƒæå’Œå¼·åŒ–',
                    implementation=self._get_security_implementation(app_type),
                    validation='é‹è¡Œ Trivy å’Œ Hadolint æƒæ'
                ))
            elif priority == 'size':
                analysis['recommendations'].append(OptimizationRecommendation(
                    category='å¤§å°å„ªåŒ–',
                    priority='é«˜',
                    impact='é«˜',
                    effort='ä½',
                    description=f'ä½¿ç”¨ {strategy["recommended_base"]} åŸºç¤æ˜ åƒ',
                    implementation=self._get_size_implementation(app_type),
                    validation='æ¯”è¼ƒæ˜ åƒå¤§å°å‰å¾Œ'
                ))
            elif priority == 'startup_time':
                analysis['recommendations'].append(OptimizationRecommendation(
                    category='å•Ÿå‹•æ€§èƒ½',
                    priority='ä¸­',
                    impact='é«˜',
                    effort='ä¸­',
                    description='å„ªåŒ–æ‡‰ç”¨ç¨‹å¼å•Ÿå‹•æ™‚é–“',
                    implementation=self._get_startup_implementation(app_type),
                    validation='æ¸¬é‡å®¹å™¨å•Ÿå‹•æ™‚é–“'
                ))
    
    def _estimate_size_savings(self, optimization_type: str) -> str:
        """ä¼°è¨ˆå„ªåŒ–çš„æ½›åœ¨å¤§å°ç¯€çœ"""
        savings_map = {
            'large_base_images': '200-800MB',
            'apt_cleanup_missing': '50-200MB',
            'yum_cleanup_missing': '100-300MB',
            'apk_cleanup_missing': '20-100MB',
            'excessive_layers': '10-50MB',
            'multi_stage_optimization': '100-500MB'
        }
        return savings_map.get(optimization_type, '10-50MB')
    
    def _get_security_implementation(self, app_type: str) -> str:
        """æ ¹æ“šæ‡‰ç”¨ç¨‹å¼é¡å‹ç²å–å®‰å…¨å¯¦æ–½"""
        implementations = {
            'web_application': 'é root ä½¿ç”¨è€…ï¼Œå®‰å…¨æƒæï¼Œæœ€å°å¥—ä»¶',
            'microservice': 'Distroless åŸºç¤ï¼Œéœæ…‹ç·¨è­¯ï¼Œèƒ½åŠ›ä¸Ÿæ£„',
            'data_processing': 'å®‰å…¨è³‡æ–™è™•ç†ï¼ŒåŠ å¯†å·ï¼Œç¶²è·¯ç­–ç•¥',
            'machine_learning': 'æ¨¡å‹åŠ å¯†ï¼Œå®‰å…¨æ¨¡å‹æœå‹™ï¼ŒGPU å®‰å…¨'
        }
        return implementations.get(app_type, 'æ¨™æº–å®‰å…¨å¼·åŒ–')
```

**é€²éšå¤šæ¡†æ¶ Dockerfile ç”Ÿæˆå™¨**
```python
class FrameworkOptimizedDockerfileGenerator:
    def __init__(self):
        self.templates = {
            'node_express': self._generate_node_express_optimized,
            'python_fastapi': self._generate_python_fastapi_optimized,
            'python_django': self._generate_python_django_optimized,
            'golang_gin': self._generate_golang_optimized,
            'java_spring': self._generate_java_spring_optimized,
            'rust_actix': self._generate_rust_optimized,
            'dotnet_core': self._generate_dotnet_optimized
        }
    
    def generate_optimized_dockerfile(self, framework: str, config: Dict[str, Any]) -> str:
        """ç‚ºç‰¹å®šæ¡†æ¶ç”Ÿæˆé«˜åº¦å„ªåŒ–çš„ Dockerfile"""
        if framework not in self.templates:
            raise ValueError(f"ä¸æ”¯æ´çš„æ¡†æ¶: {framework}")
        
        return self.templates[framework](config)
    
    def _generate_node_express_optimized(self, config: Dict) -> str:
        """
        ç”Ÿæˆå„ªåŒ–çš„ Node.js Express Dockerfile
        """
        node_version = config.get('node_version', '20')
        use_bun = config.get('use_bun', False)
        
        if use_bun:
            return f"""
# ä½¿ç”¨ Bun å„ªåŒ–çš„ Node.js - è¶…å¿«é€Ÿå»ºç½®å’Œé‹è¡Œæ™‚
FROM oven/bun:{config.get('bun_version', 'latest')} AS base

# å®‰è£ä¾è³´é … (Bun æ¯” npm å¿«å¾—å¤š)
WORKDIR /app
COPY package.json bun.lockb* ./
RUN bun install --frozen-lockfile --production

# å»ºç½®éšæ®µ
FROM base AS build
COPY . .
RUN bun run build

# ç”Ÿç”¢éšæ®µ
FROM gcr.io/distroless/nodejs{node_version}-debian11
WORKDIR /app

# è¤‡è£½å·²å»ºç½®çš„æ‡‰ç”¨ç¨‹å¼
COPY --from=build --chown=nonroot:nonroot /app/dist ./dist
COPY --from=build --chown=nonroot:nonroot /app/node_modules ./node_modules
COPY --from=build --chown=nonroot:nonroot /app/package.json ./

# å®‰å…¨ï¼šä»¥é root é‹è¡Œ
USER nonroot

# å¥åº·æª¢æŸ¥
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
    CMD ["node", "-e", "require('http').get('http://localhost:3000/health', (res) => process.exit(res.statusCode === 200 ? 0 : 1)).on('error', () => process.exit(1))"]

EXPOSE 3000
CMD ["node", "dist/index.js"]
"""
        
        return f"""
# å„ªåŒ–çš„ Node.js Express - ç”Ÿç”¢å°±ç·’çš„å¤šéšæ®µå»ºç½®
FROM node:{node_version}-alpine AS deps

# å®‰è£ dumb-init ä»¥æ­£ç¢ºè™•ç†ä¿¡è™Ÿ
RUN apk add --no-cache dumb-init

# å»ºç«‹æ‡‰ç”¨ç¨‹å¼ç›®éŒ„ä¸¦è¨­å®šé©ç•¶æ¬Šé™
RUN addgroup -g 1001 -S nodejs && adduser -S nodejs -u 1001
WORKDIR /app
USER nodejs

# è¤‡è£½å¥—ä»¶æª”æ¡ˆä¸¦å®‰è£ä¾è³´é …
COPY --chown=nodejs:nodejs package*.json ./
RUN npm ci --only=production --no-audit --no-fund && npm cache clean --force

# å»ºç½®éšæ®µ
FROM node:{node_version}-alpine AS build
WORKDIR /app
COPY package*.json ./
RUN npm ci --no-audit --no-fund
COPY . .
RUN npm run build && npm run test

# ç”Ÿç”¢éšæ®µ
FROM node:{node_version}-alpine AS production

# å®‰è£ dumb-init
RUN apk add --no-cache dumb-init

# å»ºç«‹ä½¿ç”¨è€…å’Œæ‡‰ç”¨ç¨‹å¼ç›®éŒ„
RUN addgroup -g 1001 -S nodejs && adduser -S nodejs -u 1001
WORKDIR /app
USER nodejs

# è¤‡è£½å·²å»ºç½®çš„æ‡‰ç”¨ç¨‹å¼
COPY --from=build --chown=nodejs:nodejs /app/dist ./dist
COPY --from=deps --chown=nodejs:nodejs /app/node_modules ./node_modules
COPY --from=build --chown=nodejs:nodejs /app/package.json ./

# å¥åº·æª¢æŸ¥
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
    CMD node healthcheck.js || exit 1

# æš´éœ²åŸ 
EXPOSE 3000

# ä½¿ç”¨ dumb-init ä»¥æ­£ç¢ºè™•ç†ä¿¡è™Ÿ
ENTRYPOINT ["dumb-init", "--"]
CMD ["node", "dist/index.js"]
"""
    
    def _generate_python_fastapi_optimized(self, config: Dict) -> str:
        """
        ç”Ÿæˆå„ªåŒ–çš„ Python FastAPI Dockerfile
        """
        python_version = config.get('python_version', '3.11')
        use_uv = config.get('use_uv', True)
        
        if use_uv:
            return f"""
# ä½¿ç”¨ uv å¥—ä»¶ç®¡ç†å™¨å¯¦ç¾è¶…å¿«é€Ÿ Python
FROM python:{python_version}-slim AS base

# å®‰è£ uv - æœ€å¿«çš„ Python å¥—ä»¶ç®¡ç†å™¨
RUN pip install uv

# å»ºç½®ä¾è³´é …
FROM base AS build
WORKDIR /app

# è¤‡è£½ requirements ä¸¦ä½¿ç”¨ uv å®‰è£ä¾è³´é …
COPY requirements.txt ./
RUN uv venv /opt/venv && \
    . /opt/venv/bin/activate && \
    uv pip install --no-cache-dir -r requirements.txt

# ç”Ÿç”¢éšæ®µ
FROM python:{python_version}-slim AS production

# å®‰è£å®‰å…¨æ›´æ–°
RUN apt-get update && apt-get upgrade -y && \
    apt-get install -y --no-install-recommends dumb-init && \
    rm -rf /var/lib/apt/lists/*

# å»ºç«‹é root ä½¿ç”¨è€…
RUN useradd -m -u 1001 appuser
WORKDIR /app

# è¤‡è£½è™›æ“¬ç’°å¢ƒ
COPY --from=build /opt/venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH"

# è¤‡è£½æ‡‰ç”¨ç¨‹å¼
COPY --chown=appuser:appuser . ./

# å®‰å…¨ï¼šä»¥é root é‹è¡Œ
USER appuser

# å¥åº·æª¢æŸ¥
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD python -c "import requests; requests.get('http://localhost:8000/health', timeout=5)"

EXPOSE 8000

# ä½¿ç”¨ dumb-init å’Œ Gunicorn é€²è¡Œç”Ÿç”¢
ENTRYPOINT ["dumb-init", "--"]
CMD ["gunicorn", "--bind", "0.0.0.0:8000", "--workers", "4", "--worker-class", "uvicorn.workers.UvicornWorker", "app.main:app"]
"""
        
        # æ¨™æº–å„ªåŒ– Python Dockerfile
        return f"""
# å„ªåŒ–çš„ Python FastAPI - ç”Ÿç”¢å°±ç·’
FROM python:{python_version}-slim AS build

# å®‰è£å»ºç½®ä¾è³´é …
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# å»ºç«‹è™›æ“¬ç’°å¢ƒ
RUN python -m venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH"

# å®‰è£ Python ä¾è³´é …
COPY requirements.txt ./
RUN pip install --no-cache-dir --upgrade pip && \
    pip install --no-cache-dir -r requirements.txt

# ç”Ÿç”¢éšæ®µ
FROM python:{python_version}-slim AS production

# å®‰è£é‹è¡Œæ™‚ä¾è³´é …å’Œå®‰å…¨æ›´æ–°
RUN apt-get update && apt-get install -y --no-install-recommends \
    dumb-init \
    && apt-get upgrade -y \
    && rm -rf /var/lib/apt/lists/*

# å»ºç«‹é root ä½¿ç”¨è€…
RUN useradd -m -u 1001 appuser
WORKDIR /app

# è¤‡è£½è™›æ“¬ç’°å¢ƒ
COPY --from=build /opt/venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH" \
    PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    PYTHONOPTIMIZE=2

# è¤‡è£½æ‡‰ç”¨ç¨‹å¼
COPY --chown=appuser:appuser . ./

# å®‰å…¨ï¼šä»¥é root é‹è¡Œ
USER appuser

# å¥åº·æª¢æŸ¥
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD python -c "import urllib.request; urllib.request.urlopen('http://localhost:8000/health', timeout=5)"

EXPOSE 8000

# å¸¶æœ‰æ­£ç¢ºä¿¡è™Ÿè™•ç†çš„ç”Ÿç”¢ä¼ºæœå™¨
ENTRYPOINT ["dumb-init", "--"]
CMD ["gunicorn", "--bind", "0.0.0.0:8000", "--workers", "4", "--worker-class", "uvicorn.workers.UvicornWorker", "app.main:app"]
"""
    
    def _generate_golang_optimized(self, config: Dict) -> str:
        """
        ç”Ÿæˆå„ªåŒ–çš„ Go Dockerfileï¼Œå…·æœ‰æœ€å°çš„æœ€çµ‚æ˜ åƒ
        """
        go_version = config.get('go_version', '1.21')
        
        return f"""
# å„ªåŒ–çš„ Go å»ºç½® - è¶…æœ€å°æœ€çµ‚æ˜ åƒ
FROM golang:{go_version}-alpine AS build

# å®‰è£ git ä»¥ç”¨æ–¼ Go æ¨¡çµ„
RUN apk add --no-cache git ca-certificates tzdata

# å»ºç«‹å»ºç½®ç›®éŒ„
WORKDIR /build

# è¤‡è£½ go mod æª”æ¡ˆä¸¦ä¸‹è¼‰ä¾è³´é …
COPY go.mod go.sum ./
RUN go mod download && go mod verify

# è¤‡è£½åŸå§‹ç¢¼
COPY . .

# ä½¿ç”¨å„ªåŒ–å»ºç½®éœæ…‹äºŒé€²ä½æª”
RUN CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build \
    -ldflags='-w -s -extldflags "-static"' \
    -a -installsuffix cgo \
    -o app .

# ä½¿ç”¨ UPX å£“ç¸®äºŒé€²ä½æª”ï¼ˆå¯é¸ï¼Œå¤§å°æ¸›å°‘ 50-70%ï¼‰
RUN upx --best --lzma app

# æœ€çµ‚éšæ®µ - æœ€å° scratch æ˜ åƒ
FROM scratch

# è¤‡è£½å¿…è¦çš„æª”æ¡ˆ
COPY --from=build /etc/ssl/certs/ca-certificates.crt /etc/ssl/certs/
COPY --from=build /usr/share/zoneinfo /usr/share/zoneinfo
COPY --from=build /build/app /app

# å¥åº·æª¢æŸ¥ (ä½¿ç”¨æ‡‰ç”¨ç¨‹å¼æœ¬èº«)
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
    CMD ["/app", "--health-check"]

EXPOSE 8080

# é‹è¡ŒäºŒé€²ä½æª”
ENTRYPOINT ["/app"]
"""
    
    def _check_base_image(self, content, analysis):
        """å¢å¼·çš„åŸºç¤æ˜ åƒå„ªåŒ–åˆ†æ"""
        from_match = re.search(r'^FROM\s+(.+?)(?:\s+AS\s+\w+)?$', content, re.MULTILINE)
        if from_match:
            base_image = from_match.group(1)
            
            # æª¢æŸ¥æœ€æ–°æ¨™ç±¤
            if ':latest' in base_image or not ':' in base_image:
                analysis['security_risks'].append({
                    'issue': 'ä½¿ç”¨ latest æˆ–ç„¡æ¨™ç±¤',
                    'severity': 'HIGH',
                    'fix': 'å›ºå®šåˆ°ç‰¹å®šç‰ˆæœ¬',
                    'example': f'FROM {base_image.split(":")[0]}:1.2.3',
                    'impact': 'ä¸å¯é æ¸¬çš„å»ºç½®ï¼Œå®‰å…¨æ¼æ´'
                })
            
            # å¢å¼·çš„åŸºç¤æ˜ åƒå»ºè­°
            optimization_recommendations = {
                'ubuntu': {
                    'alternatives': ['ubuntu:22.04-slim', 'debian:bullseye-slim', 'alpine:3.18'],
                    'savings': '400-600MB',
                    'notes': 'ç”Ÿç”¢ç’°å¢ƒè€ƒæ…® distroless'
                },
                'debian': {
                    'alternatives': ['debian:bullseye-slim', 'alpine:3.18', 'gcr.io/distroless/base'],
                    'savings': '300-500MB',
                    'notes': 'Distroless æä¾›æ›´å¥½çš„å®‰å…¨æ€§'
                },
                'centos': {
                    'alternatives': ['alpine:3.18', 'gcr.io/distroless/base', 'ubuntu:22.04-slim'],
                    'savings': '200-400MB',
                    'notes': 'CentOS å·²æ£„ç”¨ï¼Œé·ç§»åˆ°æ›¿ä»£æ–¹æ¡ˆ'
                },
                'node': {
                    'alternatives': ['node:20-alpine', 'node:20-slim', 'gcr.io/distroless/nodejs20'],
                    'savings': '300-700MB',
                    'notes': 'Alpine æœ€å°ï¼Œdistroless æœ€å®‰å…¨'
                },
                'python': {
                    'alternatives': ['python:3.11-slim', 'python:3.11-alpine', 'gcr.io/distroless/python3'],
                    'savings': '400-800MB',
                    'notes': 'Slim å¹³è¡¡å¤§å°å’Œç›¸å®¹æ€§'
                }
            }
            
            for base_name, config in optimization_recommendations.items():
                if base_name in base_image and 'slim' not in base_image and 'alpine' not in base_image:
                    analysis['size_impact'].append({
                        'issue': f'å¤§å‹åŸºç¤æ˜ åƒ: {base_image}',
                        'impact': config['savings'],
                        'alternatives': config['alternatives'],
                        'recommendation': f"åˆ‡æ›åˆ° {config['alternatives'][0]} ä»¥ç²å¾—æœ€ä½³å¤§å°/ç›¸å®¹æ€§å¹³è¡¡",
                        'notes': config['notes']
                    })
            
            # æª¢æŸ¥å·²æ£„ç”¨æˆ–ä¸å®‰å…¨çš„åŸºç¤æ˜ åƒ
            deprecated_images = {
                'centos:7': 'å·²é” EOLï¼Œé·ç§»åˆ° Rocky Linux æˆ– Alpine',
                'ubuntu:18.04': 'LTS å·²çµæŸï¼Œå‡ç´šåˆ° ubuntu:22.04',
                'node:14': 'Node 14 å·² EOLï¼Œå‡ç´šåˆ° node:18 æˆ– node:20',
                'python:3.8': 'Python 3.8 å³å°‡é”åˆ° EOLï¼Œå‡ç´šåˆ° 3.11+'
            }
            
            for deprecated, message in deprecated_images.items():
                if deprecated in base_image:
                    analysis['security_risks'].append({
                        'issue': f'å·²æ£„ç”¨çš„åŸºç¤æ˜ åƒ: {deprecated}',
                        'severity': 'MEDIUM',
                        'fix': message,
                        'impact': 'å®‰å…¨æ¼æ´ï¼Œç„¡å®‰å…¨æ›´æ–°'
                    })
    
    def _check_layer_optimization(self, content: str, analysis: Dict):
        """ä½¿ç”¨ç¾ä»£æœ€ä½³å¯¦è¸å¢å¼·å±¤å„ªåŒ–åˆ†æ"""
        lines = content.split('\n')
        
        # æª¢æŸ¥å¤šå€‹ RUN å‘½ä»¤
        run_commands = [line for line in lines if line.strip().startswith('RUN')]
        if len(run_commands) > 5:
            analysis['build_performance'].append({
                'issue': f'éå¤šçš„ RUN å‘½ä»¤ ({len(run_commands)})',
                'impact': f'å»ºç«‹ {len(run_commands)} å€‹ä¸å¿…è¦çš„å±¤',
                'fix': 'ä½¿ç”¨ && åˆä½µç›¸é—œçš„ RUN å‘½ä»¤',
                'optimization': f'å¯ä»¥æ¸›å°‘åˆ° 2-3 å±¤ï¼Œç¯€çœç´„ {len(run_commands) * 10}MB'
            })
        
        # å¢å¼·çš„å¥—ä»¶ç®¡ç†å™¨æ¸…ç†æª¢æŸ¥
        package_managers = {
            'apt': {
                'install_pattern': r'RUN.*apt-get.*install',
                'cleanup_pattern': r'rm\s+-rf\s+/var/lib/apt/lists',
                'update_pattern': r'apt-get\s+update',
                'combined_check': r'RUN.*apt-get\s+update.*&&.*apt-get\s+install.*&&.*rm\s+-rf\s+/var/lib/apt/lists/*',
                'recommended_pattern': 'RUN apt-get update && apt-get install -y --no-install-recommends <packages> && rm -rf /var/lib/apt/lists/*'
            },
            'yum': {
                'install_pattern': r'RUN.*yum.*install',
                'cleanup_pattern': r'yum\s+clean\s+all',
                'recommended_pattern': 'RUN yum install -y <packages> && yum clean all'
            },
            'apk': {
                'install_pattern': r'RUN.*apk.*add',
                'cleanup_pattern': r'--no-cache|rm\s+-rf\s+/var/cache/apk',
                'recommended_pattern': 'RUN apk add --no-cache <packages>'
            },
            'pip': {
                'install_pattern': r'RUN.*pip.*install',
                'cleanup_pattern': r'--no-cache-dir|pip\s+cache\s+purge',
                'recommended_pattern': 'RUN pip install --no-cache-dir <packages>'
            }
        }
        
        for pm_name, patterns in package_managers.items():
            has_install = re.search(patterns['install_pattern'], content)
            has_cleanup = re.search(patterns['cleanup_pattern'], content)
            
            if has_install and not has_cleanup:
                potential_savings = {
                    'apt': '50-200MB',
                    'yum': '100-300MB', 
                    'apk': '5-50MB',
                    'pip': '20-100MB'
                }.get(pm_name, '10-50MB')
                
                analysis['size_impact'].append({
                    'issue': f'{pm_name} å¥—ä»¶ç®¡ç†å™¨ç¼ºå°‘æ¸…ç†',
                    'impact': potential_savings,
                    'fix': 'åœ¨åŒä¸€ RUN å‘½ä»¤ä¸­æ·»åŠ æ¸…ç†',
                    'example': patterns['recommended_pattern'],
                    'severity': 'MEDIUM'
                })
        
        # æª¢æŸ¥ä½æ•ˆçš„ COPY æ“ä½œ
        copy_commands = [line for line in lines if line.strip().startswith(('COPY', 'ADD'))]
        for cmd in copy_commands:
            if 'COPY . .' in cmd or 'COPY ./ ./' in cmd:
                analysis['build_performance'].append({
                    'issue': 'ä½æ•ˆçš„ COPY å‘½ä»¤è¤‡è£½æ•´å€‹ä¸Šä¸‹æ–‡',
                    'impact': 'å»ºç½®å¿«å–æ•ˆç‡ä½ï¼Œå»ºç½®é€Ÿåº¦æ…¢',
                    'fix': 'ä½¿ç”¨ç‰¹å®šçš„ COPY å‘½ä»¤å’Œ .dockerignore',
                    'example': 'COPY package*.json ./ && COPY src/ ./src/',
                    'note': 'å…ˆè¤‡è£½ä¾è³´é …æª”æ¡ˆä»¥ç²å¾—æ›´å¥½çš„å¿«å–'
                })
        
        # æª¢æŸ¥ BuildKit å„ªåŒ–
        if '--mount=type=cache' not in content:
            analysis['build_performance'].append({
                'issue': 'ç¼ºå°‘ BuildKit å¿«å–æ›è¼‰',
                'impact': 'å»ºç½®é€Ÿåº¦æ…¢ï¼Œç„¡ä¾è³´é …å¿«å–',
                'fix': 'ä½¿ç”¨ BuildKit å¿«å–æ›è¼‰ç”¨æ–¼å¥—ä»¶ç®¡ç†å™¨',
                'example': 'RUN --mount=type=cache,target=/root/.cache/pip pip install -r requirements.txt',
                'note': 'éœ€è¦ DOCKER_BUILDKIT=1'
            })
        
        # æª¢æŸ¥å¤šéšæ®µå»ºç½®æ©Ÿæœƒ
        from_statements = re.findall(r'FROM\s+([^\s]+)', content)
        if len(from_statements) == 1 and any(keyword in content.lower() for keyword in ['build', 'compile', 'npm install', 'pip install']):
            analysis['size_impact'].append({
                'issue': 'å–®éšæ®µå»ºç½®èˆ‡é–‹ç™¼ä¾è³´é …',
                'impact': '100-500MB ä¾†è‡ªå»ºç½®å·¥å…·å’Œé–‹ç™¼ä¾è³´é …',
                'fix': 'å¯¦æ–½å¤šéšæ®µå»ºç½®',
                'example': 'åˆ†é›¢å»ºç½®å’Œé‹è¡Œæ™‚éšæ®µ',
                'potential_savings': '200-800MB'
            })
```

### 2. é€²éšå¤šéšæ®µå»ºç½®ç­–ç•¥

å¯¦æ–½è¤‡é›œçš„å¤šéšæ®µå»ºç½®ï¼Œä¸¦æ¡ç”¨ç¾ä»£å„ªåŒ–æŠ€è¡“ï¼š

**è¶…å„ªåŒ–å¤šéšæ®µæ¨¡å¼**
```dockerfile
# æ¨¡å¼ 1ï¼šå¸¶æœ‰ Bun çš„ Node.js - ä¸‹ä¸€ä»£ JavaScript é‹è¡Œæ™‚
# å®‰è£é€Ÿåº¦å¿« 5 å€ï¼Œé‹è¡Œæ™‚å¿« 4 å€ï¼Œæ˜ åƒå° 90%
FROM oven/bun:1.0-alpine AS base

# éšæ®µ 1ï¼šä½¿ç”¨ Bun é€²è¡Œä¾è³´é …è§£æ
FROM base AS deps
WORKDIR /app

# Bun é–å®šæª”æ¡ˆç”¨æ–¼ç¢ºå®šæ€§å»ºç½®
COPY package.json bun.lockb* ./

# è¶…å¿«é€Ÿä¾è³´é …å®‰è£
RUN bun install --frozen-lockfile --production

# éšæ®µ 2ï¼šä½¿ç”¨é–‹ç™¼ä¾è³´é …å»ºç½®
FROM base AS build
WORKDIR /app

# è¤‡è£½å¥—ä»¶æª”æ¡ˆ
COPY package.json bun.lockb* ./

# å®‰è£æ‰€æœ‰ä¾è³´é …ï¼ˆåŒ…æ‹¬é–‹ç™¼ï¼‰
RUN bun install --frozen-lockfile

# è¤‡è£½åŸå§‹ç¢¼ä¸¦å»ºç½®
COPY . .
RUN bun run build && bun test

# éšæ®µ 3ï¼šå®‰å…¨æƒæï¼ˆå¯é¸ä½†å»ºè­°ï¼‰
FROM build AS security-scan
RUN apk add --no-cache curl
# ä¸‹è¼‰ä¸¦é‹è¡Œ Trivy é€²è¡Œæ¼æ´æƒæ
RUN curl -sfL https://raw.githubusercontent.com/aquasecurity/trivy/main/contrib/install.sh | sh -s -- -b /usr/local/bin && \
    trivy fs --exit-code 1 --no-progress --severity HIGH,CRITICAL /app

# éšæ®µ 4ï¼šä½¿ç”¨ distroless å¯¦ç¾è¶…æœ€å°ç”Ÿç”¢
FROM gcr.io/distroless/nodejs20-debian11 AS production

# åƒ…è¤‡è£½ç”Ÿç”¢æ‰€éœ€çš„å…§å®¹
COPY --from=deps --chown=nonroot:nonroot /app/node_modules ./node_modules
COPY --from=build --chown=nonroot:nonroot /app/dist ./dist
COPY --from=build --chown=nonroot:nonroot /app/package.json ./

# Distroless å·²ä»¥é root é‹è¡Œ
USER nonroot

# å¥åº·æª¢æŸ¥
HEALTHCHECK --interval=30s --timeout=5s --start-period=10s --retries=3 \
    CMD ["node", "-e", "require('http').get('http://localhost:3000/health',(r)=>process.exit(r.statusCode===200?0:1)).on('error',()=>process.exit(1))"]

EXPOSE 3000
CMD ["node", "dist/index.js"]
```

**ä½¿ç”¨ UV å¥—ä»¶ç®¡ç†å™¨å¯¦ç¾é€²éš Python å¤šéšæ®µ**
```dockerfile
# æ¨¡å¼ 2ï¼šå¸¶æœ‰ UV çš„ Python - æ¯” pip å¿« 10-100 å€
FROM python:3.11-slim AS base

# å®‰è£ UV - ä¸‹ä¸€ä»£ Python å¥—ä»¶ç®¡ç†å™¨
RUN pip install uv

# éšæ®µ 1ï¼šä½¿ç”¨ UV é€²è¡Œä¾è³´é …è§£æ
FROM base AS deps
WORKDIR /app

# è¤‡è£½ requirements
COPY requirements.txt requirements-dev.txt ./

# å»ºç«‹è™›æ“¬ç’°å¢ƒä¸¦ä½¿ç”¨ UV å®‰è£ç”Ÿç”¢ä¾è³´é …
RUN uv venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH"
RUN uv pip install --no-cache-dir -r requirements.txt

# éšæ®µ 2ï¼šå»ºç½®å’Œæ¸¬è©¦
FROM base AS build
WORKDIR /app

# å®‰è£æ‰€æœ‰ä¾è³´é …ï¼ŒåŒ…æ‹¬é–‹ç™¼
RUN uv venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH"
COPY requirements*.txt ./
RUN uv pip install --no-cache-dir -r requirements.txt -r requirements-dev.txt

# è¤‡è£½åŸå§‹ç¢¼ä¸¦é‹è¡Œæ¸¬è©¦
COPY . .
RUN python -m pytest tests/ --cov=src --cov-report=term-missing
RUN python -m black --check src/
RUN python -m isort --check-only src/
RUN python -m mypy src/

# éšæ®µ 3ï¼šå®‰å…¨å’Œåˆè¦æ€§æƒæ
FROM build AS security
RUN uv pip install safety bandit
RUN safety check
RUN bandit -r src/ -f json -o bandit-report.json

# éšæ®µ 4ï¼šä½¿ç”¨ distroless å„ªåŒ–ç”Ÿç”¢
FROM gcr.io/distroless/python3-debian11 AS production

# è¤‡è£½è™›æ“¬ç’°å¢ƒå’Œæ‡‰ç”¨ç¨‹å¼
COPY --from=deps /opt/venv /opt/venv
COPY --from=build /app/src ./src
COPY --from=build /app/requirements.txt ./

# è¨­å®šç”Ÿç”¢ç’°å¢ƒ
ENV PATH="/opt/venv/bin:$PATH" \
    PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    PYTHONOPTIMIZE=2

# å¥åº·æª¢æŸ¥
HEALTHCHECK --interval=30s --timeout=10s --start-period=10s --retries=3 \
    CMD ["python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8000/health', timeout=5)"]

EXPOSE 8000
CMD ["python", "-m", "gunicorn", "--bind", "0.0.0.0:8000", "--workers", "4", "--worker-class", "uvicorn.workers.UvicornWorker", "src.main:app"]
```

**å¸¶æœ‰ Scratch åŸºç¤çš„ Go éœæ…‹äºŒé€²ä½æª”**
```dockerfile
# æ¨¡å¼ 3ï¼šå¸¶æœ‰è¶…æœ€å° scratch åŸºç¤çš„ Go
FROM golang:1.21-alpine AS base

# å®‰è£ git ä»¥ç”¨æ–¼ Go æ¨¡çµ„
RUN apk add --no-cache git ca-certificates tzdata

# å»ºç«‹å»ºç½®ç›®éŒ„
WORKDIR /build

# è¤‡è£½ go mod æª”æ¡ˆä¸¦ä¸‹è¼‰ä¾è³´é …
COPY go.mod go.sum ./
RUN go mod download && go mod verify

# è¤‡è£½åŸå§‹ç¢¼
COPY . .

# ä½¿ç”¨å„ªåŒ–å»ºç½®éœæ…‹äºŒé€²ä½æª”
RUN CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build \
    -ldflags='-w -s -extldflags "-static"' \
    -a -installsuffix cgo \
    -o app .

# ä½¿ç”¨ UPX å£“ç¸®äºŒé€²ä½æª”ï¼ˆå¯é¸ï¼Œå¤§å°æ¸›å°‘ 50-70%ï¼‰
RUN upx --best --lzma app

# æœ€çµ‚éšæ®µ - æœ€å° scratch æ˜ åƒ
FROM scratch

# è¤‡è£½å¿…è¦çš„æª”æ¡ˆ
COPY --from=build /etc/ssl/certs/ca-certificates.crt /etc/ssl/certs/
COPY --from=build /usr/share/zoneinfo /usr/share/zoneinfo
COPY --from=build /build/app /app

# å¥åº·æª¢æŸ¥ (ä½¿ç”¨æ‡‰ç”¨ç¨‹å¼æœ¬èº«å…§å»ºçš„å¥åº·ç«¯é»)
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
    CMD ["/app", "--health-check"]

EXPOSE 8080

# é‹è¡ŒäºŒé€²ä½æª”
ENTRYPOINT ["/app"]
```

**å¸¶æœ‰äº¤å‰ç·¨è­¯å’Œå®‰å…¨çš„ Rust**
```dockerfile
# æ¨¡å¼ 4ï¼šå¸¶æœ‰ musl çš„ Rust ç”¨æ–¼éœæ…‹é€£çµ
FROM rust:1.70-alpine AS base

# å®‰è£ musl é–‹ç™¼å·¥å…·
RUN apk add --no-cache musl-dev openssl-dev

# éšæ®µ 1ï¼šä¾è³´é …å¿«å–
FROM base AS deps
WORKDIR /app

# è¤‡è£½ Cargo æª”æ¡ˆ
COPY Cargo.toml Cargo.lock ./

# å»ºç«‹è™›æ“¬ main ä¸¦å»ºç½®ä¾è³´é …
RUN mkdir src && echo 'fn main() {}' > src/main.rs
RUN --mount=type=cache,target=/usr/local/cargo/registry \
    --mount=type=cache,target=/app/target \
    cargo build --release --target x86_64-unknown-linux-musl

# éšæ®µ 2ï¼šå»ºç½®æ‡‰ç”¨ç¨‹å¼
FROM base AS build
WORKDIR /app

# è¤‡è£½å¿«å–ä¸­çš„ä¾è³´é …
COPY --from=deps /usr/local/cargo /usr/local/cargo
COPY . .

# å»ºç½®å„ªåŒ–éœæ…‹äºŒé€²ä½æª”
RUN --mount=type=cache,target=/usr/local/cargo/registry \
    --mount=type=cache,target=/app/target \
    cargo build --release --target x86_64-unknown-linux-musl && \
    cp target/x86_64-unknown-linux-musl/release/app /app/app

# å‰é›¢äºŒé€²ä½æª”ä»¥æ¸›å°å¤§å°
RUN strip /app/app

# éšæ®µ 3ï¼šå®‰å…¨æƒæ
FROM build AS security
RUN cargo audit
RUN cargo clippy -- -D warnings

# éšæ®µ 4ï¼šæœ€å° scratch æ˜ åƒ
FROM scratch AS production

# è¤‡è£½éœæ…‹äºŒé€²ä½æª”
COPY --from=build /app/app /app

# å¥åº·æª¢æŸ¥
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
    CMD ["/app", "--health"]

EXPOSE 8000
ENTRYPOINT ["/app"]
```

**å¸¶æœ‰ GraalVM Native Image çš„ Java Spring Boot**
```dockerfile
# æ¨¡å¼ 5ï¼šå¸¶æœ‰ GraalVM Native Image çš„ Javaï¼ˆäºç§’ç´šå•Ÿå‹•ï¼‰
FROM ghcr.io/graalvm/graalvm-ce:java17 AS base

# å®‰è£ native-image çµ„ä»¶
RUN gu install native-image

# éšæ®µ 1ï¼šä¾è³´é …
FROM base AS deps
WORKDIR /app

# è¤‡è£½ Maven/Gradle æª”æ¡ˆ
COPY pom.xml ./
COPY .mvn .mvn
COPY mvnw ./

# ä¸‹è¼‰ä¾è³´é …
RUN ./mvnw dependency:go-offline

# éšæ®µ 2ï¼šå»ºç½®æ‡‰ç”¨ç¨‹å¼
FROM base AS build
WORKDIR /app

# è¤‡è£½ä¾è³´é …å’ŒåŸå§‹ç¢¼
COPY --from=deps /root/.m2 /root/.m2
COPY . .

# å»ºç½® JAR
RUN ./mvnw clean package -DskipTests

# å»ºç½®åŸç”Ÿæ˜ åƒ
RUN native-image \
    -jar target/*.jar \
    --no-fallback \
    --static \
    --libc=musl \
    -H:+ReportExceptionStackTraces \
    -H:+AddAllCharsets \
    -H:IncludeResourceBundles=sun.util.resources.TimeZoneNames \
    app

# éšæ®µ 3ï¼šæ¸¬è©¦
FROM build AS test
RUN ./mvnw test

# éšæ®µ 4ï¼šè¶…æœ€å°æœ€çµ‚æ˜ åƒï¼ˆ20-50MB vs 200-300MBï¼‰
FROM scratch AS production

# è¤‡è£½åŸç”ŸäºŒé€²ä½æª”
COPY --from=build /app/app /app

# å¥åº·æª¢æŸ¥
HEALTHCHECK --interval=30s --timeout=3s --start-period=2s --retries=3 \
    CMD ["/app", "--health"]

EXPOSE 8080
ENTRYPOINT ["/app"]
```

**Python å¤šéšæ®µç¯„ä¾‹**
```dockerfile
# éšæ®µ 1ï¼šå»ºç½®ä¾è³´é …
FROM python:3.11-slim AS builder

# å®‰è£å»ºç½®ä¾è³´é …
RUN apt-get update && apt-get install -y --no-install-recommends \
    gcc \
    libc6-dev \
    && rm -rf /var/lib/apt/lists/*

# å»ºç«‹è™›æ“¬ç’°å¢ƒ
RUN python -m venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH"

# å®‰è£ Python ä¾è³´é …
COPY requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt

# éšæ®µ 2ï¼šé‹è¡Œæ™‚
FROM python:3.11-slim AS runtime

# å¾å»ºç½®å™¨è¤‡è£½è™›æ“¬ç’°å¢ƒ
COPY --from=builder /opt/venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH"

# å»ºç«‹é root ä½¿ç”¨è€…
RUN useradd -m -u 1001 appuser

WORKDIR /app

# è¤‡è£½æ‡‰ç”¨ç¨‹å¼
COPY --chown=appuser:appuser . ./

USER appuser

# Gunicorn ç”¨æ–¼ç”Ÿç”¢
CMD ["gunicorn", "--bind", "0.0.0.0:8000", "--workers", "4", "app:application"]
```

### 3. æ˜ åƒå¤§å°å„ªåŒ–

æœ€å°åŒ– Docker æ˜ åƒå¤§å°ï¼š

**å¤§å°ç¸®æ¸›æŠ€è¡“**
```dockerfile
# åŸºæ–¼ Alpine çš„å„ªåŒ–
FROM alpine:3.18

# åƒ…å®‰è£å¿…è¦çš„å¥—ä»¶
RUN apk add --no-cache \
    python3 \
    py3-pip \
    && pip3 install --no-cache-dir --upgrade pip

# å°æ–¼ pip ä½¿ç”¨ --no-cache-dir
COPY requirements.txt ./
RUN pip3 install --no-cache-dir -r requirements.txt

# ç§»é™¤ä¸å¿…è¦çš„æª”æ¡ˆ
RUN find /usr/local -type d -name __pycache__ -exec rm -rf {} + \
    && find /usr/local -type f -name '*.pyc' -delete

# å¸¶æœ‰ scratch æ˜ åƒçš„ Golang ç¯„ä¾‹
FROM golang:1.21-alpine AS builder
WORKDIR /build
COPY . .
# å»ºç½®éœæ…‹äºŒé€²ä½æª”
RUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -ldflags '-s -w' -o app .

# æœ€çµ‚éšæ®µï¼šscratch
FROM scratch
# åƒ…è¤‡è£½äºŒé€²ä½æª”
COPY --from=builder /build/app /app
# è¤‡è£½ SSL æ†‘è­‰ä»¥ç”¨æ–¼ HTTPS
COPY --from=builder /etc/ssl/certs/ca-certificates.crt /etc/ssl/certs/
ENTRYPOINT ["/app"]
```

**å±¤å„ªåŒ–è…³æœ¬**
```python
def optimize_dockerfile_layers(dockerfile_content):
    """
    å„ªåŒ– Dockerfile å±¤
    """
    optimizations = []
    
    # åˆä½µ RUN å‘½ä»¤
    run_commands = re.findall(r'^RUN\s+(.+?)(?=^(?:RUN|FROM|COPY|ADD|ENV|EXPOSE|CMD|ENTRYPOINT|WORKDIR)|\Z)', 
                             dockerfile_content, re.MULTILINE | re.DOTALL)
    
    if len(run_commands) > 1:
        combined = ' && \
    '.join(cmd.strip() for cmd in run_commands)
        optimizations.append({
            'original': '\n'.join(f'RUN {cmd}' for cmd in run_commands),
            'optimized': f'RUN {combined}',
            'benefit': f'å°‡ {len(run_commands)} å±¤æ¸›å°‘åˆ° 1 å±¤'
        })
    
    # å„ªåŒ–å¥—ä»¶å®‰è£
    apt_install = re.search(r'RUN\s+apt-get\s+update.*?apt-get\s+install\s+(.+?)(?=^(?:RUN|FROM)|\Z)', 
                           dockerfile_content, re.MULTILINE | re.DOTALL)
    
    if apt_install:
        packages = apt_install.group(1)
        optimized = f"""RUN apt-get update && apt-get install -y --no-install-recommends \
    {packages.strip()} \
    && rm -rf /var/lib/apt/lists/*"""
        
        optimizations.append({
            'original': apt_install.group(0),
            'optimized': optimized,
            'benefit': 'é€éæ¸…ç† apt å¿«å–æ¸›å°‘æ˜ åƒå¤§å°'
        })
    
    return optimizations
```

### 4. å»ºç½®æ€§èƒ½å„ªåŒ–

åŠ é€Ÿ Docker å»ºç½®ï¼š

**.dockerignore å„ªåŒ–**
```
# .dockerignore
# ç‰ˆæœ¬æ§åˆ¶
.git
.gitignore

# é–‹ç™¼
.vscode
.idea
*.swp
*.swo

# ä¾è³´é …
node_modules
vendor
venv
__pycache__

# å»ºç½®å·¥ä»¶
dist
build
*.egg-info
target

# æ¸¬è©¦
test
tests
*.test.js
*.spec.js
coverage
.pytest_cache

# æ–‡ä»¶
docs
*.md
LICENSE

# ç’°å¢ƒ
.env
.env.*

# æ—¥èªŒ
*.log
logs

# ä½œæ¥­ç³»çµ±æª”æ¡ˆ
.DS_Store
Thumbs.db

# CI/CD
.github
.gitlab
.circleci
Jenkinsfile

# Docker
Dockerfile*
docker-compose*
.dockerignore
```

**å»ºç½®å¿«å–å„ªåŒ–**
```dockerfile
# å„ªåŒ–å»ºç½®å¿«å–
FROM node:18-alpine

WORKDIR /app

# å…ˆè¤‡è£½å¥—ä»¶æª”æ¡ˆï¼ˆæ›´æ”¹é »ç‡è¼ƒä½ï¼‰
COPY package*.json ./

# å®‰è£ä¾è³´é …ï¼ˆå¦‚æœå¥—ä»¶æª”æ¡ˆæœªæ›´æ”¹ï¼Œå‰‡å¿«å–ï¼‰
RUN npm ci --only=production

# è¤‡è£½åŸå§‹ç¢¼ï¼ˆæ›´æ”¹é »ç‡è¼ƒé«˜ï¼‰
COPY . .

# å»ºç½®æ‡‰ç”¨ç¨‹å¼
RUN npm run build

# ä½¿ç”¨ BuildKit å¿«å–æ›è¼‰
FROM node:18-alpine AS builder
WORKDIR /app

# æ›è¼‰å¥—ä»¶ç®¡ç†å™¨å¿«å–
RUN --mount=type=cache,target=/root/.npm \
    npm ci --only=production

# æ›è¼‰å»ºç½®å·¥ä»¶å¿«å–
RUN --mount=type=cache,target=/app/.cache \
    npm run build
```

### 5. å®‰å…¨å¼·åŒ–

å¯¦æ–½å®‰å…¨æœ€ä½³å¯¦è¸ï¼š

**å®‰å…¨å¼·åŒ– Dockerfile**
```dockerfile
# ä½¿ç”¨ç‰¹å®šç‰ˆæœ¬å’Œæœ€å°åŸºç¤æ˜ åƒ
FROM alpine:3.18.4

# å®‰è£å®‰å…¨æ›´æ–°
RUN apk update && apk upgrade && apk add --no-cache \
    ca-certificates \
    && rm -rf /var/cache/apk/*

# å»ºç«‹é root ä½¿ç”¨è€…
RUN addgroup -g 1001 -S appgroup && \
    adduser -S appuser -u 1001 -G appgroup

# è¨­å®šå®‰å…¨æ¬Šé™
RUN mkdir /app && chown -R appuser:appgroup /app
WORKDIR /app

# ä»¥æ­£ç¢ºçš„æ‰€æœ‰æ¬Šè¤‡è£½
COPY --chown=appuser:appgroup . .

# ä¸Ÿæ£„æ‰€æœ‰èƒ½åŠ›
USER appuser

# åªè®€æ ¹æª”æ¡ˆç³»çµ±
# ç‚ºå¯å¯«ç›®éŒ„æ·»åŠ å·
VOLUME ["/tmp", "/app/logs"]

# å®‰å…¨æ¨™ç±¤
LABEL security.scan="trivy" \
      security.updates="auto"

# å¸¶æœ‰è¶…æ™‚çš„å¥åº·æª¢æŸ¥
HEALTHCHECK --interval=30s --timeout=3s --retries=3 \
    CMD wget --no-verbose --tries=1 --spider http://localhost:8080/health || exit 1

# ä»¥ PID 1 é‹è¡Œä»¥æ­£ç¢ºè™•ç†ä¿¡è™Ÿ
ENTRYPOINT ["dumb-init", "--"]
CMD ["./app"]
```

**å®‰å…¨æƒææ•´åˆ**
```yaml
# .github/workflows/docker-security.yml
name: Docker å®‰å…¨æƒæ

on:
  push:
    paths:
      - 'Dockerfile*'
      - '.dockerignore'

jobs:
  scan:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: é‹è¡Œ Trivy æ¼æ´æƒæå™¨
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: '${{ github.repository }}:${{ github.sha }}'
          format: 'sarif'
          output: 'trivy-results.sarif'
          severity: 'CRITICAL,HIGH'
          
      - name: ä¸Šå‚³ Trivy æƒæçµæœ
        uses: github/codeql-action/upload-sarif@v2
        with:
          sarif_file: 'trivy-results.sarif'
          
      - name: é‹è¡Œ Hadolint
        uses: hadolint/hadolint-action@v3.1.0
        with:
          dockerfile: Dockerfile
          format: sarif
          output-file: hadolint-results.sarif
          
      - name: ä¸Šå‚³ Hadolint æƒæçµæœ
        uses: github/codeql-action/upload-sarif@v2
        with:
          sarif_file: hadolint-results.sarif
```

### 6. é‹è¡Œæ™‚å„ªåŒ–

å„ªåŒ–å®¹å™¨é‹è¡Œæ™‚æ€§èƒ½ï¼š

**é‹è¡Œæ™‚é…ç½®**
```dockerfile
# JVM å„ªåŒ–ç¯„ä¾‹
FROM eclipse-temurin:17-jre-alpine

# æ ¹æ“šå®¹å™¨é™åˆ¶è¨­å®š JVM è¨˜æ†¶é«”
ENV JAVA_OPTS="-XX:MaxRAMPercentage=75.0 \
    -XX:InitialRAMPercentage=50.0 \
    -XX:+UseContainerSupport \
    -XX:+OptimizeStringConcat \
    -XX:+UseStringDeduplication \
    -Djava.security.egd=file:/dev/./urandom"

# Node.js å„ªåŒ–
FROM node:18-alpine
ENV NODE_ENV=production \
    NODE_OPTIONS="--max-old-space-size=1024 --optimize-for-size"

# Python å„ªåŒ–
FROM python:3.11-slim
ENV PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    PYTHONOPTIMIZE=2

# Nginx å„ªåŒ–
FROM nginx:alpine
COPY nginx-optimized.conf /etc/nginx/nginx.conf
# å•Ÿç”¨ gzipã€å¿«å–å’Œé€£æ¥æ± 
```

### 7. Docker Compose å„ªåŒ–

å„ªåŒ–å¤šå®¹å™¨æ‡‰ç”¨ç¨‹å¼ï¼š

```yaml
# docker-compose.yml
version: '3.9'

services:
  app:
    build:
      context: .
      dockerfile: Dockerfile
      cache_from:
        - ${REGISTRY}/app:latest
        - ${REGISTRY}/app:builder
      args:
        BUILDKIT_INLINE_CACHE: 1
    image: ${REGISTRY}/app:${VERSION:-latest}
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 512M
        reservations:
          cpus: '0.5'
          memory: 256M
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    restart: unless-stopped
    
  redis:
    image: redis:7-alpine
    command: redis-server --maxmemory 256mb --maxmemory-policy allkeys-lru
    deploy:
      resources:
        limits:
          memory: 256M
    volumes:
      - type: tmpfs
        target: /data
        tmpfs:
          size: 268435456 # 256MB
          
  nginx:
    image: nginx:alpine
    volumes:
      - type: bind
        source: ./nginx.conf
        target: /etc/nginx/nginx.conf
        read_only: true
    depends_on:
      app:
        condition: service_healthy
```

### 8. å»ºç½®è‡ªå‹•åŒ–

è‡ªå‹•åŒ–å„ªåŒ–å»ºç½®ï¼š

**å»ºç½®è‡ªå‹•åŒ–è…³æœ¬**
```bash
#!/bin/bash
# build-optimize.sh

set -euo pipefail

# è®Šæ•¸
IMAGE_NAME="${1:-myapp}"
VERSION="${2:-latest}"
PLATFORMS="${3:-linux/amd64,linux/arm64}"

echo "ğŸ—ï¸ æ­£åœ¨å»ºç½®å„ªåŒ– Docker æ˜ åƒ..."

# å•Ÿç”¨ BuildKit
export DOCKER_BUILDKIT=1

# å¸¶å¿«å–å»ºç½®
docker buildx build \
  --platform "${PLATFORMS}" \
  --cache-from "type=registry,ref=${IMAGE_NAME}:buildcache" \
  --cache-to "type=registry,ref=${IMAGE_NAME}:buildcache,mode=max" \
  --tag "${IMAGE_NAME}:${VERSION}" \
  --build-arg BUILDKIT_INLINE_CACHE=1 \
  --progress=plain \
  --push \
  .

# åˆ†ææ˜ åƒå¤§å°
echo "ğŸ“Š æ˜ åƒåˆ†æï¼š"
docker run --rm -v /var/run/docker.sock:/var/run/docker.sock \
  wagoodman/dive:latest "${IMAGE_NAME}:${VERSION}"

# å®‰å…¨æƒæ
echo "ğŸ”’ å®‰å…¨æƒæï¼š"
trivy image "${IMAGE_NAME}:${VERSION}"

# å¤§å°å ±å‘Š
echo "ğŸ“ å¤§å°æ¯”è¼ƒï¼š"
docker images "${IMAGE_NAME}" --format "table {{.Repository}}	{{.Tag}}	{{.Size}}"
```

### 9. ç›£æ§å’ŒæŒ‡æ¨™

è¿½è¹¤å®¹å™¨æ€§èƒ½ï¼š

```python
# container-metrics.py
import docker
import json
from datetime import datetime

class ContainerMonitor:
    def __init__(self):
        self.client = docker.from_env()
        
    def collect_metrics(self, container_name):
        """æ”¶é›†å®¹å™¨æ€§èƒ½æŒ‡æ¨™"""
        container = self.client.containers.get(container_name)
        stats = container.stats(stream=False)
        
        metrics = {
            'timestamp': datetime.now().isoformat(),
            'container': container_name,
            'cpu': self._calculate_cpu_percent(stats),
            'memory': self._calculate_memory_usage(stats),
            'network': self._calculate_network_io(stats),
            'disk': self._calculate_disk_io(stats)
        }
        
        return metrics
    
    def _calculate_cpu_percent(self, stats):
        """è¨ˆç®— CPU ä½¿ç”¨ç‡ç™¾åˆ†æ¯”"""
        cpu_delta = stats['cpu_stats']['cpu_usage']['total_usage'] - \
                   stats['precpu_stats']['cpu_usage']['total_usage']
        system_delta = stats['cpu_stats']['system_cpu_usage'] - \
                      stats['precpu_stats']['system_cpu_usage']
        
        if system_delta > 0 and cpu_delta > 0:
            cpu_percent = (cpu_delta / system_delta) * \
                         len(stats['cpu_stats']['cpu_usage']['percpu_usage']) * 100.0
            return round(cpu_percent, 2)
        return 0.0
    
    def _calculate_memory_usage(self, stats):
        """è¨ˆç®—è¨˜æ†¶é«”ä½¿ç”¨é‡"""
        usage = stats['memory_stats']['usage']
        limit = stats['memory_stats']['limit']
        
        return {
            'usage_bytes': usage,
            'limit_bytes': limit,
            'percent': round((usage / limit) * 100, 2)
        }
```

### 10. æœ€ä½³å¯¦è¸æª¢æŸ¥æ¸…å–®

```python
def generate_dockerfile_checklist():
    """ç”Ÿæˆ Dockerfile æœ€ä½³å¯¦è¸æª¢æŸ¥æ¸…å–®"""
    checklist = """
## Dockerfile æœ€ä½³å¯¦è¸æª¢æŸ¥æ¸…å–®

### åŸºç¤æ˜ åƒ
- [ ] ä½¿ç”¨ç‰¹å®šç‰ˆæœ¬æ¨™ç±¤ï¼ˆè€Œé :latestï¼‰
- [ ] ä½¿ç”¨æœ€å°åŸºç¤æ˜ åƒï¼ˆalpineã€slimã€distrolessï¼‰
- [ ] ä¿æŒåŸºç¤æ˜ åƒæ›´æ–°
- [ ] ç›¡å¯èƒ½ä½¿ç”¨å®˜æ–¹æ˜ åƒ

### å±¤å’Œå¿«å–
- [ ] å‘½ä»¤é †åºå¾æœ€ä¸é »ç¹æ›´æ”¹åˆ°æœ€é »ç¹æ›´æ”¹
- [ ] é©ç•¶æ™‚åˆä½µ RUN å‘½ä»¤
- [ ] åœ¨åŒä¸€å±¤ä¸­æ¸…ç†ï¼ˆapt å¿«å–ã€pip å¿«å–ï¼‰
- [ ] ä½¿ç”¨ .dockerignore æ’é™¤ä¸å¿…è¦çš„æª”æ¡ˆ

### å®‰å…¨
- [ ] ä»¥é root ä½¿ç”¨è€…é‹è¡Œ
- [ ] ä¸åœ¨æ˜ åƒä¸­å„²å­˜ç§˜å¯†
- [ ] æƒææ˜ åƒä»¥æŸ¥æ‰¾æ¼æ´
- [ ] ä½¿ç”¨ COPY è€Œé ADD
- [ ] ç›¡å¯èƒ½è¨­å®šåªè®€æ ¹æª”æ¡ˆç³»çµ±

### å¤§å°å„ªåŒ–
- [ ] ä½¿ç”¨å¤šéšæ®µå»ºç½®
- [ ] ç§»é™¤ä¸å¿…è¦çš„ä¾è³´é …
- [ ] æ¸…é™¤å¥—ä»¶ç®¡ç†å™¨å¿«å–
- [ ] ç§»é™¤è‡¨æ™‚æª”æ¡ˆå’Œå»ºç½®å·¥ä»¶
- [ ] å°æ–¼ apt ä½¿ç”¨ --no-install-recommends

### æ€§èƒ½
- [ ] è¨­å®šé©ç•¶çš„è³‡æºé™åˆ¶
- [ ] ä½¿ç”¨å¥åº·æª¢æŸ¥
- [ ] å„ªåŒ–å•Ÿå‹•æ™‚é–“
- [ ] é©ç•¶é…ç½®æ—¥èªŒè¨˜éŒ„
- [ ] ä½¿ç”¨ BuildKit åŠ é€Ÿå»ºç½®

### å¯ç¶­è­·æ€§
- [ ] åŒ…å« LABEL å…ƒè³‡æ–™
- [ ] ä½¿ç”¨ EXPOSE è¨˜éŒ„æš´éœ²çš„åŸ 
- [ ] ä½¿ç”¨ ARG é€²è¡Œå»ºç½®æ™‚è®Šæ•¸
- [ ] åŒ…å«æœ‰æ„ç¾©çš„è¨»é‡‹
- [ ] ç‰ˆæœ¬åŒ–æ‚¨çš„ Dockerfile
"""
    return checklist
```

## è¼¸å‡ºæ ¼å¼

1. **åˆ†æå ±å‘Š**ï¼šç•¶å‰ Dockerfile å•é¡Œå’Œå„ªåŒ–æ©Ÿæœƒ
2. **å„ªåŒ– Dockerfile**ï¼šé‡å¯«çš„ Dockerfileï¼ŒåŒ…å«æ‰€æœ‰å„ªåŒ–
3. **å¤§å°æ¯”è¼ƒ**ï¼šæ˜ åƒå¤§å°å‰å¾Œåˆ†æ
4. **å»ºç½®æ€§èƒ½**ï¼šå»ºç½®æ™‚é–“æ”¹é€²å’Œå¿«å–ç­–ç•¥
5. **å®‰å…¨å ±å‘Š**ï¼šå®‰å…¨æƒæçµæœå’Œå¼·åŒ–å»ºè­°
6. **é‹è¡Œæ™‚é…ç½®**ï¼šæ‡‰ç”¨ç¨‹å¼çš„å„ªåŒ–é‹è¡Œæ™‚è¨­å®š
7. **ç›£æ§è¨­å®š**ï¼šå®¹å™¨æŒ‡æ¨™å’Œæ€§èƒ½è¿½è¹¤
8. **é·ç§»æŒ‡å—**ï¼šå¯¦æ–½å„ªåŒ–çš„é€æ­¥æŒ‡å—

## è·¨æŒ‡ä»¤æ•´åˆ

### å®Œæ•´çš„å®¹å™¨å„ªå…ˆé–‹ç™¼å·¥ä½œæµç¨‹

**å®¹å™¨åŒ–é–‹ç™¼ç®¡é“**
```bash
# 1. ç”Ÿæˆå®¹å™¨åŒ– API è…³æ‰‹æ¶
/api-scaffold
framework: "fastapi"
deployment_target: "kubernetes"
containerization: true
monitoring: true

# 2. å„ªåŒ–å®¹å™¨ä»¥ç”¨æ–¼ç”Ÿç”¢
/docker-optimize
optimization_level: "production"
security_hardening: true
multi_stage_build: true

# 3. å®‰å…¨æƒæå®¹å™¨æ˜ åƒ
/security-scan
scan_types: ["container", "dockerfile", "runtime"]
image_name: "app:optimized"
generate_sbom: true

# 4. ç‚ºå„ªåŒ–å®¹å™¨ç”Ÿæˆ K8s æ¸…å–®
/k8s-manifest
container_security: "strict"
resource_optimization: true
horizontal_scaling: true
```

**æ•´åˆå®¹å™¨é…ç½®**
```python
# container-config.py - æ‰€æœ‰æŒ‡ä»¤å…±äº«
class IntegratedContainerConfig:
    def __init__(self):
        self.api_config = self.load_api_config()           # ä¾†è‡ª /api-scaffold
        self.security_config = self.load_security_config() # ä¾†è‡ª /security-scan
        self.k8s_config = self.load_k8s_config()          # ä¾†è‡ª /k8s-manifest
        self.test_config = self.load_test_config()         # ä¾†è‡ª /test-harness
        
    def generate_optimized_dockerfile(self):
        """ç”Ÿæˆé‡å°ç‰¹å®šæ‡‰ç”¨ç¨‹å¼å„ªåŒ–çš„ Dockerfile"""
        framework = self.api_config.get('framework', 'python')
        security_level = self.security_config.get('level', 'standard')
        deployment_target = self.k8s_config.get('platform', 'kubernetes')
        
        if framework == 'fastapi':
            return self.generate_fastapi_dockerfile(security_level, deployment_target)
        elif framework == 'express':
            return self.generate_express_dockerfile(security_level, deployment_target)
        elif framework == 'django':
            return self.generate_django_dockerfile(security_level, deployment_target)
            
    def generate_fastapi_dockerfile(self, security_level, deployment_target):
        """ç”Ÿæˆå„ªåŒ–çš„ FastAPI Dockerfile"""
        dockerfile_content = {
            'base_image': self.select_base_image('python', security_level),
            'build_stages': self.configure_build_stages(),
            'security_configs': self.apply_security_configurations(security_level),
            'runtime_optimizations': self.configure_runtime_optimizations(),
            'monitoring_setup': self.configure_monitoring_setup(),
            'health_checks': self.configure_health_checks()
        }
        return dockerfile_content
    
    def select_base_image(self, language, security_level):
        """æ ¹æ“šå®‰å…¨å’Œå¤§å°è¦æ±‚é¸æ“‡æœ€ä½³åŸºç¤æ˜ åƒ"""
        base_images = {
            'python': {
                'minimal': 'python:3.11-alpine',
                'standard': 'python:3.11-slim-bookworm',
                'secure': 'chainguard/python:latest-dev',
                'distroless': 'gcr.io/distroless/python3-debian12'
            }
        }
        
        if security_level == 'strict':
            return base_images[language]['distroless']
        elif security_level == 'enhanced':
            return base_images[language]['secure']
        else:
            return base_images[language]['standard']
    
    def configure_build_stages(self):
        """é…ç½®å¤šéšæ®µå»ºç½®å„ªåŒ–"""
        return {
            'dependencies_stage': {
                'name': 'dependencies',
                'base': 'python:3.11-slim-bookworm',
                'actions': [
                    'COPY requirements.txt .',
                    'RUN pip install --no-cache-dir --user -r requirements.txt'
                ]
            },
            'security_stage': {
                'name': 'security-scan',
                'base': 'dependencies',
                'actions': [
                    'RUN pip-audit --format=json --output=/tmp/security-report.json',
                    'RUN safety check --json --output=/tmp/safety-report.json'
                ]
            },
            'runtime_stage': {
                'name': 'runtime',
                'base': 'python:3.11-slim-bookworm',
                'actions': [
                    'COPY --from=dependencies /root/.local /root/.local',
                    'COPY --from=security-scan /tmp/*-report.json /security-reports/'
                ]
            }
        }
```

**API å®¹å™¨æ•´åˆ**
```dockerfile
# Dockerfile.api - å¾ /api-scaffold + /docker-optimize ç”Ÿæˆ
# ç‚º FastAPI æ‡‰ç”¨ç¨‹å¼å„ªåŒ–çš„å¤šéšæ®µå»ºç½®
FROM python:3.11-slim-bookworm AS base

# è¨­å®šç’°å¢ƒè®Šæ•¸ä»¥é€²è¡Œå„ªåŒ–
ENV PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    PIP_NO_CACHE_DIR=1 \
    PIP_DISABLE_PIP_VERSION_CHECK=1

# éšæ®µ 1ï¼šä¾è³´é …
FROM base AS dependencies
WORKDIR /app

# å®‰è£ç”¨æ–¼å»ºç½® Python å¥—ä»¶çš„ç³»çµ±ä¾è³´é …
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# è¤‡è£½ä¸¦å®‰è£ Python ä¾è³´é …
COPY requirements.txt ./
RUN pip install --user --no-warn-script-location -r requirements.txt

# éšæ®µ 2ï¼šå®‰å…¨æƒæ
FROM dependencies AS security-scan
RUN pip install --user pip-audit safety bandit

# è¤‡è£½åŸå§‹ç¢¼ä»¥é€²è¡Œå®‰å…¨æƒæ
COPY . .

# åœ¨å»ºç½®æœŸé–“é‹è¡Œå®‰å…¨æƒæ
RUN python -m bandit -r . -f json -o /tmp/bandit-report.json || true
RUN python -m safety check --json --output /tmp/safety-report.json || true
RUN python -m pip_audit --format=json --output=/tmp/pip-audit-report.json || true

# éšæ®µ 3ï¼šæ¸¬è©¦ï¼ˆå¯é¸ï¼Œå¯åœ¨ç”Ÿç”¢å»ºç½®ä¸­è·³éï¼‰
FROM security-scan AS testing
RUN pip install --user pytest pytest-cov

# åœ¨å»ºç½®æœŸé–“é‹è¡Œæ¸¬è©¦ï¼ˆä¾†è‡ª /test-harness æ•´åˆï¼‰
RUN python -m pytest tests/ --cov=src --cov-report=json --cov-report=term

# éšæ®µ 4ï¼šç”Ÿç”¢é‹è¡Œæ™‚
FROM base AS runtime

# å»ºç«‹é root ä½¿ç”¨è€…ä»¥æé«˜å®‰å…¨æ€§
RUN groupadd -r appuser && useradd -r -g appuser appuser

# è¨­å®šæ‡‰ç”¨ç¨‹å¼ç›®éŒ„
WORKDIR /app
RUN chown appuser:appuser /app

# è¤‡è£½ Python å¥—ä»¶å¾ä¾è³´é …éšæ®µ
COPY --from=dependencies --chown=appuser:appuser /root/.local /home/appuser/.local

# è¤‡è£½å®‰å…¨å ±å‘Šå¾å®‰å…¨æƒæéšæ®µ
COPY --from=security-scan /tmp/*-report.json /app/security-reports/

# è¤‡è£½æ‡‰ç”¨ç¨‹å¼ç¨‹å¼ç¢¼
COPY --chown=appuser:appuser . .

# æ›´æ–° PATH ä»¥åŒ…å«ä½¿ç”¨è€…å¥—ä»¶
ENV PATH=/home/appuser/.local/bin:$PATH

# åˆ‡æ›åˆ°é root ä½¿ç”¨è€…
USER appuser

# é…ç½®å¥åº·æª¢æŸ¥ï¼ˆèˆ‡ K8s å¥åº·æª¢æŸ¥æ•´åˆï¼‰
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD python -c "import requests; requests.get('http://localhost:8000/health', timeout=5)"

# æš´éœ²åŸ ï¼ˆå¾ API è…³æ‰‹æ¶é…ç½®ï¼‰
EXPOSE 8000

# è¨­å®šæœ€ä½³å•Ÿå‹•å‘½ä»¤
CMD ["python", "-m", "uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "4"]
```

**è³‡æ–™åº«å®¹å™¨æ•´åˆ**
```dockerfile
# Dockerfile.db - ç‚ºä¾†è‡ª /db-migrate çš„è³‡æ–™åº«é·ç§»ç”Ÿæˆ
FROM postgres:15-alpine AS base

# å®‰è£é·ç§»å·¥å…·
RUN apk add --no-cache python3 py3-pip
RUN pip3 install alembic psycopg2-binary

# å»ºç«‹é·ç§»ä½¿ç”¨è€…
RUN addgroup -g 1001 migration && adduser -D -u 1001 -G migration migration

# éšæ®µ 1ï¼šé·ç§»æº–å‚™
FROM base AS migration-prep
WORKDIR /migrations

# å¾ /db-migrate è¼¸å‡ºè¤‡è£½é·ç§»è…³æœ¬
COPY --chown=migration:migration migrations/ ./
COPY --chown=migration:migration alembic.ini ./

# é©—è­‰é·ç§»è…³æœ¬
USER migration
RUN alembic check || echo "é·ç§»é©—è­‰å®Œæˆ"

# éšæ®µ 2ï¼šç”Ÿç”¢è³‡æ–™åº«
FROM postgres:15-alpine AS production

# è¤‡è£½å·²é©—è­‰çš„é·ç§»
COPY --from=migration-prep --chown=postgres:postgres /migrations /docker-entrypoint-initdb.d/

# é…ç½® PostgreSQL ä»¥ç”¨æ–¼ç”Ÿç”¢
RUN echo "shared_preload_libraries = 'pg_stat_statements'" >> /usr/local/share/postgresql/postgresql.conf.sample
RUN echo "track_activity_query_size = 2048" >> /usr/local/share/postgresql/postgresql.conf.sample
RUN echo "log_min_duration_statement = 1000" >> /usr/local/share/postgresql/postgresql.conf.sample

# ä¾†è‡ª /security-scan çš„å®‰å…¨é…ç½®
RUN echo "ssl = on" >> /usr/local/share/postgresql/postgresql.conf.sample
RUN echo "log_connections = on" >> /usr/local/share/postgresql/postgresql.conf.sample
RUN echo "log_disconnections = on" >> /usr/local/share/postgresql/postgresql.conf.sample

EXPOSE 5432
```

**å‰ç«¯å®¹å™¨æ•´åˆ**
```dockerfile
# Dockerfile.frontend - å¾ /frontend-optimize + /docker-optimize ç”Ÿæˆ
# ç‚º React/Vue æ‡‰ç”¨ç¨‹å¼å„ªåŒ–çš„å¤šéšæ®µå»ºç½®
FROM node:18-alpine AS base

# è¨­å®šç’°å¢ƒè®Šæ•¸
ENV NODE_ENV=production \
    NPM_CONFIG_CACHE=/tmp/.npm

# éšæ®µ 1ï¼šä¾è³´é …
FROM base AS dependencies
WORKDIR /app

# è¤‡è£½å¥—ä»¶æª”æ¡ˆ
COPY package*.json ./

# å®‰è£ä¾è³´é …ä¸¦é€²è¡Œå„ªåŒ–
RUN npm ci --only=production --silent

# éšæ®µ 2ï¼šå»ºç½®æ‡‰ç”¨ç¨‹å¼
FROM base AS build
WORKDIR /app

# è¤‡è£½ä¾è³´é …
COPY --from=dependencies /app/node_modules ./node_modules

# è¤‡è£½åŸå§‹ç¢¼
COPY . .

# ä½¿ç”¨ /frontend-optimize çš„å„ªåŒ–å»ºç½®æ‡‰ç”¨ç¨‹å¼
RUN npm run build

# é‹è¡Œå®‰å…¨å¯©è¨ˆ
RUN npm audit --audit-level high --production

# éšæ®µ 3ï¼šå®‰å…¨æƒæ
FROM build AS security-scan

# å®‰è£å®‰å…¨æƒæå·¥å…·
RUN npm install -g retire snyk

# é‹è¡Œå®‰å…¨æƒæ
RUN retire --outputformat json --outputpath /tmp/retire-report.json || true
RUN snyk test --json > /tmp/snyk-report.json || true

# éšæ®µ 4ï¼šç”Ÿç”¢ä¼ºæœå™¨
FROM nginx:alpine AS production

# å®‰è£å®‰å…¨æ›´æ–°
RUN apk update && apk upgrade && apk add --no-cache dumb-init

# å»ºç«‹é root ä½¿ç”¨è€…
RUN addgroup -g 1001 www && adduser -D -u 1001 -G www www

# è¤‡è£½å·²å»ºç½®çš„æ‡‰ç”¨ç¨‹å¼
COPY --from=build --chown=www:www /app/dist /usr/share/nginx/html

# è¤‡è£½å®‰å…¨å ±å‘Š
COPY --from=security-scan /tmp/*-report.json /var/log/security/

# è¤‡è£½å„ªåŒ–çš„ nginx é…ç½®
COPY nginx.conf /etc/nginx/nginx.conf

# é…ç½®æ­£ç¢ºçš„æª”æ¡ˆæ¬Šé™
RUN chown -R www:www /usr/share/nginx/html
RUN chmod -R 755 /usr/share/nginx/html

# ä½¿ç”¨é root ä½¿ç”¨è€…
USER www

# å‰ç«¯å¥åº·æª¢æŸ¥
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
    CMD wget --no-verbose --tries=1 --spider http://localhost:80/ || exit 1

EXPOSE 80

# ä½¿ç”¨ dumb-init ä»¥æ­£ç¢ºè™•ç†ä¿¡è™Ÿ
ENTRYPOINT ["dumb-init", "--"]
CMD ["nginx", "-g", "daemon off;"]
```

**Kubernetes å®¹å™¨æ•´åˆ**
```yaml
# k8s-optimized-deployment.yaml - å¾ /k8s-manifest + /docker-optimize ç”Ÿæˆ
apiVersion: v1
kind: ConfigMap
metadata:
  name: container-config
  namespace: production
data:
  optimization-level: "production"
  security-level: "strict"
  monitoring-enabled: "true"

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: optimized-api
  namespace: production
  labels:
    app: api
    optimization: enabled
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
      maxSurge: 1
  selector:
    matchLabels:
      app: api
  template:
    metadata:
      labels:
        app: api
      annotations:
        # å®¹å™¨å„ªåŒ–è¨»é‡‹
        container.seccomp.security.alpha.kubernetes.io/defaultProfileName: runtime/default
        container.apparmor.security.beta.kubernetes.io/api: runtime/default
    spec:
      # å„ªåŒ– Pod é…ç½®
      securityContext:
        runAsNonRoot: true
        runAsUser: 1001
        runAsGroup: 1001
        fsGroup: 1001
        seccompProfile:
          type: RuntimeDefault
      
      # ä¾†è‡ªå®¹å™¨åˆ†æçš„è³‡æºå„ªåŒ–
      containers:
      - name: api
        image: registry.company.com/api:optimized-latest
        imagePullPolicy: Always
        
        # å„ªåŒ–è³‡æºåˆ†é…
        resources:
          requests:
            memory: "128Mi"     # æ ¹æ“šå¯¦éš›ä½¿ç”¨æƒ…æ³å„ªåŒ–
            cpu: "100m"         # æ ¹æ“šè² è¼‰æ¸¬è©¦å„ªåŒ–
            ephemeral-storage: "1Gi"
          limits:
            memory: "512Mi"     # é˜²æ­¢ OOMï¼Œå…è¨±çªç™¼
            cpu: "500m"         # å…è¨±è™•ç†å³°å€¼
            ephemeral-storage: "2Gi"
        
        # å®¹å™¨å®‰å…¨å„ªåŒ–
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          runAsNonRoot: true
          runAsUser: 1001
          capabilities:
            drop:
              - ALL
            add:
              - NET_BIND_SERVICE
        
        # å„ªåŒ–å•Ÿå‹•å’Œå¥åº·æª¢æŸ¥
        ports:
        - containerPort: 8000
          protocol: TCP
          
        # å¿«é€Ÿå•Ÿå‹•æ¢é‡
        startupProbe:
          httpGet:
            path: /startup
            port: 8000
          failureThreshold: 30
          periodSeconds: 1
          
        # å„ªåŒ–å¥åº·æª¢æŸ¥
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
          
        readinessProbe:
          httpGet:
            path: /ready
            port: 8000
          initialDelaySeconds: 2

```

**CI/CD å®¹å™¨æ•´åˆ**
```yaml
# .github/workflows/container-pipeline.yml
name: Optimized Container Pipeline

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  build-and-optimize:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write
      security-events: write
    
    strategy:
      matrix:
        service: [api, frontend, database]
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    # 1. Build multi-stage container
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
      with:
        driver-opts: network=host
    
    - name: Log in to Container Registry
      uses: docker/login-action@v3
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}
    
    # 2. Build optimized images
    - name: Build and push container images
      uses: docker/build-push-action@v5
      with:
        context: .
        file: Dockerfile.${{ matrix.service }}
        push: true
        tags: |
          ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}/${{ matrix.service }}:${{ github.sha }}
          ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}/${{ matrix.service }}:latest
        cache-from: type=gha
        cache-to: type=gha,mode=max
        platforms: linux/amd64,linux/arm64
    
    # 3. Container security scanning
    - name: Run Trivy vulnerability scanner
      uses: aquasecurity/trivy-action@master
      with:
        image-ref: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}/${{ matrix.service }}:${{ github.sha }}
        format: 'sarif'
        output: 'trivy-results-${{ matrix.service }}.sarif'
    
    # 4. Container optimization analysis
    - name: Analyze container optimization
      run: |
        docker images --format "table {{.Repository}}	{{.Tag}}	{{.Size}}" | \
        grep ${{ matrix.service }} > container-analysis-${{ matrix.service }}.txt
        
        # Compare with baseline
        if [ -f baseline-sizes.txt ]; then
          echo "Size comparison for ${{ matrix.service }}:" >> size-comparison.txt
          echo "Previous: $(grep ${{ matrix.service }} baseline-sizes.txt || echo 'N/A')" >> size-comparison.txt
          echo "Current: $(grep ${{ matrix.service }} container-analysis-${{ matrix.service }}.txt)" >> size-comparison.txt
        fi
    
    # 5. Performance testing
    - name: Container performance testing
      run: |
        # Start container for performance testing
        docker run -d --name test-${{ matrix.service }} \
          ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}/${{ matrix.service }}:${{ github.sha }}
        
        # Wait for startup
        sleep 30
        
        # Run basic performance tests
        if [ "${{ matrix.service }}" = "api" ]; then
          docker exec test-${{ matrix.service }} \
            python -c "import requests; print(requests.get('http://localhost:8000/health').status_code)"
        fi
        
        # Cleanup
        docker stop test-${{ matrix.service }}
        docker rm test-${{ matrix.service }}
    
    # 6. Upload security results
    - name: Upload Trivy scan results to GitHub Security tab
      uses: github/codeql-action/upload-sarif@v2
      with:
        sarif_file: 'trivy-results-${{ matrix.service }}.sarif'
    
    # 7. Generate optimization report
    - name: Generate optimization report
      run: |
        cat > optimization-report-${{ matrix.service }}.md << EOF
        # Container Optimization Report - ${{ matrix.service }}
        
        ## Build Information
        - **Image**: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}/${{ matrix.service }}:${{ github.sha }}
        - **Build Date**: $(date)
        - **Platforms**: linux/amd64,linux/arm64
        
        ## Size Analysis
        $(cat container-analysis-${{ matrix.service }}.txt)
        
        ## Security Scan
        - **Scanner**: Trivy
        - **Results**: See Security tab for detailed findings
        
        ## Optimizations Applied
        - Multi-stage build for minimal image size
        - Security hardening with non-root user
        - Layer caching for faster builds
        - Health checks for reliability
        EOF
    
    - name: Upload optimization report
      uses: actions/upload-artifact@v3
      with:
        name: optimization-report-${{ matrix.service }}
        path: optimization-report-${{ matrix.service }}.md

  deploy-to-staging:
    needs: build-and-optimize
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/develop'
    
    steps:
    - name: Deploy to staging
      run: |
        # Update K8s manifests with new image tags
        # Apply optimized K8s configurations
        kubectl set image deployment/optimized-api \
          api=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}/api:${{ github.sha }} \
          --namespace=staging
```

**Monitoring Integration**
```python
# container_monitoring.py - Integrated container monitoring
import docker
import psutil
from prometheus_client import CollectorRegistry, Gauge, Counter, Histogram
from typing import Dict, Any

class ContainerOptimizationMonitor:
    """Monitor container performance and optimization metrics"""
    
    def __init__(self):
        self.docker_client = docker.from_env()
        self.registry = CollectorRegistry()
        
        # Metrics from container optimization
        self.container_size_gauge = Gauge(
            'container_image_size_bytes', 
            'Container image size in bytes',
            ['service', 'optimization_level'],
            registry=self.registry
        )
        
        self.container_startup_time = Histogram(
            'container_startup_seconds',
            'Container startup time in seconds',
            ['service'],
            registry=self.registry
        )
        
        self.resource_usage_gauge = Gauge(
            'container_resource_usage_ratio',
            'Container resource usage ratio (used/limit)',
            ['service', 'resource_type'],
            registry=self.registry
        )
    
    def monitor_optimization_metrics(self):
        """Monitor container optimization effectiveness"""
        containers = self.docker_client.containers.list()
        
        optimization_metrics = {}
        
        for container in containers:
            service_name = container.labels.get('app', 'unknown')
            
            # Monitor image size efficiency
            image = container.image
            size_mb = self.get_image_size(image.id) / (1024 * 1024)
            
            # Monitor resource efficiency
            stats = container.stats(stream=False)
            memory_usage = self.calculate_memory_efficiency(stats)
            cpu_usage = self.calculate_cpu_efficiency(stats)
            
            # Monitor startup performance
            startup_time = self.get_container_startup_time(container)
            
            optimization_metrics[service_name] = {
                'image_size_mb': size_mb,
                'memory_efficiency': memory_usage,
                'cpu_efficiency': cpu_usage,
                'startup_time_seconds': startup_time,
                'optimization_score': self.calculate_optimization_score(
                    size_mb, memory_usage, cpu_usage, startup_time
                )
            }
            
            # Update Prometheus metrics
            self.container_size_gauge.labels(
                service=service_name,
                optimization_level='production'
            ).set(size_mb)
            
            self.container_startup_time.labels(
                service=service_name
            ).observe(startup_time)
        
        return optimization_metrics
    
    def calculate_optimization_score(self, size_mb, memory_eff, cpu_eff, startup_time):
        """Calculate overall optimization score (0-100)"""
        size_score = max(0, 100 - (size_mb / 10))  # Penalty for large images
        memory_score = (1 - memory_eff) * 100      # Reward for efficient memory use
        cpu_score = (1 - cpu_eff) * 100           # Reward for efficient CPU use
        startup_score = max(0, 100 - startup_time * 10)  # Penalty for slow startup
        
        return (size_score + memory_score + cpu_score + startup_score) / 4
```

This comprehensive integration ensures containers are optimized across the entire development lifecycle, from build-time optimization through runtime monitoring and Kubernetes deployment.
```